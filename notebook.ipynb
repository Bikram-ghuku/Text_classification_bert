{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"95269eed","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2025-06-01T15:36:00.665397Z","iopub.execute_input":"2025-06-01T15:36:00.665625Z","iopub.status.idle":"2025-06-01T15:36:05.443909Z","shell.execute_reply.started":"2025-06-01T15:36:00.665587Z","shell.execute_reply":"2025-06-01T15:36:05.443114Z"},"trusted":true},"outputs":[],"execution_count":1},{"id":"d91948e3","cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"id":"dddd92e8","cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\", index_col=0)","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:36.998500Z","iopub.execute_input":"2025-05-18T14:12:36.998799Z","iopub.status.idle":"2025-05-18T14:12:38.780206Z","shell.execute_reply.started":"2025-05-18T14:12:36.998778Z","shell.execute_reply":"2025-05-18T14:12:38.779404Z"},"trusted":true},"outputs":[],"execution_count":6},{"id":"8ae6fbf0","cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:38.781397Z","iopub.execute_input":"2025-05-18T14:12:38.781709Z","iopub.status.idle":"2025-05-18T14:12:38.792130Z","shell.execute_reply.started":"2025-05-18T14:12:38.781690Z","shell.execute_reply":"2025-05-18T14:12:38.791485Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                   model_a              model_b  \\\nid                                                \n30192   gpt-4-1106-preview           gpt-4-0613   \n53567            koala-13b           gpt-4-0613   \n65089   gpt-3.5-turbo-0613       mistral-medium   \n96401     llama-2-13b-chat  mistral-7b-instruct   \n198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                                   prompt  \\\nid                                                          \n30192   [\"Is it morally right to try to have a certain...   \n53567   [\"What is the difference between marriage lice...   \n65089   [\"explain function calling. how would you call...   \n96401   [\"How can I create a test set for a very rare ...   \n198779  [\"What is the best way to travel from Tel-Aviv...   \n\n                                               response_a  \\\nid                                                          \n30192   [\"The question of whether it is morally right ...   \n53567   [\"A marriage license is a legal document that ...   \n65089   [\"Function calling is the process of invoking ...   \n96401   [\"Creating a test set for a very rare category...   \n198779  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                               response_b  winner_model_a  \\\nid                                                                          \n30192   [\"As an AI, I don't have personal beliefs or o...               1   \n53567   [\"A marriage license and a marriage certificat...               0   \n65089   [\"Function calling is the process of invoking ...               0   \n96401   [\"When building a classifier for a very rare c...               1   \n198779  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n        winner_model_b  winner_tie  \nid                                  \n30192                0           0  \n53567                1           0  \n65089                0           1  \n96401                0           0  \n198779               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30192</th>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53567</th>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>65089</th>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96401</th>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>198779</th>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"id":"fdb03074","cell_type":"markdown","source":"# Process the dataset to remove null","metadata":{}},{"id":"5dae2004","cell_type":"code","source":"train[\"prompt\"] = train.prompt.map(lambda x: eval(x)[0])\ntrain[\"response_a\"] = train.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ntrain[\"response_b\"] = train.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:42.297717Z","iopub.execute_input":"2025-05-18T14:12:42.298425Z","iopub.status.idle":"2025-05-18T14:12:45.864323Z","shell.execute_reply.started":"2025-05-18T14:12:42.298397Z","shell.execute_reply":"2025-05-18T14:12:45.863764Z"},"trusted":true},"outputs":[],"execution_count":8},{"id":"19369909","cell_type":"markdown","source":"# Generate categorical data","metadata":{}},{"id":"36951a47","cell_type":"code","source":"train[\"class_name\"] = train[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ntrain[\"class_label\"] = train.class_name.map({'winner_model_a':0, 'winner_model_b': 1, 'winner_tie' : 2})\n\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:45.865462Z","iopub.execute_input":"2025-05-18T14:12:45.865750Z","iopub.status.idle":"2025-05-18T14:12:45.893330Z","shell.execute_reply.started":"2025-05-18T14:12:45.865727Z","shell.execute_reply":"2025-05-18T14:12:45.892678Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                   model_a              model_b  \\\nid                                                \n30192   gpt-4-1106-preview           gpt-4-0613   \n53567            koala-13b           gpt-4-0613   \n65089   gpt-3.5-turbo-0613       mistral-medium   \n96401     llama-2-13b-chat  mistral-7b-instruct   \n198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                                   prompt  \\\nid                                                          \n30192   Is it morally right to try to have a certain p...   \n53567   What is the difference between marriage licens...   \n65089   explain function calling. how would you call a...   \n96401   How can I create a test set for a very rare ca...   \n198779  What is the best way to travel from Tel-Aviv t...   \n\n                                               response_a  \\\nid                                                          \n30192   The question of whether it is morally right to...   \n53567   A marriage license is a legal document that al...   \n65089   Function calling is the process of invoking or...   \n96401   Creating a test set for a very rare category c...   \n198779  The best way to travel from Tel Aviv to Jerusa...   \n\n                                               response_b  winner_model_a  \\\nid                                                                          \n30192   As an AI, I don't have personal beliefs or opi...               1   \n53567   A marriage license and a marriage certificate ...               0   \n65089   Function calling is the process of invoking a ...               0   \n96401   When building a classifier for a very rare cat...               1   \n198779  The best way to travel from Tel-Aviv to Jerusa...               0   \n\n        winner_model_b  winner_tie      class_name  class_label  \nid                                                               \n30192                0           0  winner_model_a            0  \n53567                1           0  winner_model_b            1  \n65089                0           1      winner_tie            2  \n96401                0           0  winner_model_a            0  \n198779               1           0  winner_model_b            1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30192</th>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53567</th>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>65089</th>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>winner_tie</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96401</th>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>198779</th>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"id":"ad5be7a7","cell_type":"markdown","source":"# Make pair by joining the prompt and response","metadata":{}},{"id":"29a6e91a","cell_type":"code","source":"def make_pairs(row):\n    row[\"encode_fail\"] = False\n    try:\n        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n    except Exception:\n        prompt = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n    except Exception:\n        response_a = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n    except Exception:\n        response_b = \"\"\n        row[\"encode_fail\"] = True\n        \n    row['options'] = [f\"Prompt: {prompt}\\n\\nResponse: {response_a}\",\n                      f\"Prompt: {prompt}\\n\\nResponse: {response_b}\"\n                     ]\n    return row","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:48.215335Z","iopub.execute_input":"2025-05-18T14:12:48.215804Z","iopub.status.idle":"2025-05-18T14:12:48.220726Z","shell.execute_reply.started":"2025-05-18T14:12:48.215782Z","shell.execute_reply":"2025-05-18T14:12:48.220085Z"},"trusted":true},"outputs":[],"execution_count":10},{"id":"91aca6ec","cell_type":"code","source":"train = train.apply(make_pairs, axis=1)  \ntrain.head(2)","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:12:49.465695Z","iopub.execute_input":"2025-05-18T14:12:49.466509Z","iopub.status.idle":"2025-05-18T14:13:34.842731Z","shell.execute_reply.started":"2025-05-18T14:12:49.466475Z","shell.execute_reply":"2025-05-18T14:13:34.842071Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                  model_a     model_b  \\\nid                                      \n30192  gpt-4-1106-preview  gpt-4-0613   \n53567           koala-13b  gpt-4-0613   \n\n                                                  prompt  \\\nid                                                         \n30192  Is it morally right to try to have a certain p...   \n53567  What is the difference between marriage licens...   \n\n                                              response_a  \\\nid                                                         \n30192  The question of whether it is morally right to...   \n53567  A marriage license is a legal document that al...   \n\n                                              response_b  winner_model_a  \\\nid                                                                         \n30192  As an AI, I don't have personal beliefs or opi...               1   \n53567  A marriage license and a marriage certificate ...               0   \n\n       winner_model_b  winner_tie      class_name  class_label  encode_fail  \\\nid                                                                            \n30192               0           0  winner_model_a            0        False   \n53567               1           0  winner_model_b            1        False   \n\n                                                 options  \nid                                                        \n30192  [Prompt: Is it morally right to try to have a ...  \n53567  [Prompt: What is the difference between marria...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n      <th>encode_fail</th>\n      <th>options</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30192</th>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>False</td>\n      <td>[Prompt: Is it morally right to try to have a ...</td>\n    </tr>\n    <tr>\n      <th>53567</th>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>False</td>\n      <td>[Prompt: What is the difference between marria...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"id":"8d1ff1cb","cell_type":"markdown","source":"# Drop failed UTF-8 checking","metadata":{}},{"id":"48ecaa32","cell_type":"code","source":"print(train.encode_fail.value_counts(normalize=False))\ntrain = train[train['encode_fail'] == False]\nprint(train.encode_fail.value_counts(normalize=False))\n","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:14:14.852965Z","iopub.execute_input":"2025-05-18T14:14:14.853591Z","iopub.status.idle":"2025-05-18T14:14:14.879922Z","shell.execute_reply.started":"2025-05-18T14:14:14.853569Z","shell.execute_reply":"2025-05-18T14:14:14.879346Z"},"trusted":true},"outputs":[{"name":"stdout","text":"encode_fail\nFalse    56885\nTrue       592\nName: count, dtype: int64\nencode_fail\nFalse    56885\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"id":"95e335a4","cell_type":"markdown","source":"# Remove unwanted columns","metadata":{}},{"id":"1ce0eee6","cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Import package\n\ntrain = train.drop(['model_a', 'model_b', 'prompt', 'response_a','response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'class_name'], axis=1)\n\ntrain_df, valid_df = train_test_split(train, test_size=0.2, stratify=train[\"class_label\"])","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:14:16.866342Z","iopub.execute_input":"2025-05-18T14:14:16.866874Z","iopub.status.idle":"2025-05-18T14:14:17.407181Z","shell.execute_reply.started":"2025-05-18T14:14:16.866850Z","shell.execute_reply":"2025-05-18T14:14:17.406650Z"},"trusted":true},"outputs":[],"execution_count":13},{"id":"e884b6cc","cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:14:19.097366Z","iopub.execute_input":"2025-05-18T14:14:19.097824Z","iopub.status.idle":"2025-05-18T14:14:19.102946Z","shell.execute_reply.started":"2025-05-18T14:14:19.097801Z","shell.execute_reply":"2025-05-18T14:14:19.102380Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(45508, 3)"},"metadata":{}}],"execution_count":14},{"id":"a737fff3","cell_type":"markdown","source":"# Make match dataloader","metadata":{}},{"id":"e44c4966","cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, dataSet: pd.DataFrame, tokenizer):\n        self.labels = dataSet['class_label'].values\n        self.values = dataSet['options'].values\n        self.dataSet = dataSet\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, index):\n        text = self.values[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            text,\n            max_length=256,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = torch.reshape(encoding['input_ids'], (-1, ))\n        attention_mask = torch.reshape(encoding['attention_mask'], (-1, ))\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:14:24.470030Z","iopub.execute_input":"2025-05-18T14:14:24.470557Z","iopub.status.idle":"2025-05-18T14:14:24.476156Z","shell.execute_reply.started":"2025-05-18T14:14:24.470533Z","shell.execute_reply":"2025-05-18T14:14:24.475580Z"},"trusted":true},"outputs":[],"execution_count":15},{"id":"64ea0ed2","cell_type":"markdown","source":"# Load Models for tokeniser and classification from Hugging face","metadata":{}},{"id":"9d70ef2f","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=3\n)\nmodel","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:14:28.633732Z","iopub.execute_input":"2025-05-18T14:14:28.634501Z","iopub.status.idle":"2025-05-18T14:14:52.162921Z","shell.execute_reply.started":"2025-05-18T14:14:28.634467Z","shell.execute_reply":"2025-05-18T14:14:52.162350Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b9aa49baeb4976aecd04b3dfb934dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a8fa9d53a34e3fa9c7ff6a036edca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016188f7bf9b4b098408c50d152c9692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf73bac238844ec89451dab4c4f9756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34633c315f4543f08e9712ecc06659ce"}},"metadata":{}},{"name":"stderr","text":"2025-05-18 14:14:38.469037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747577678.678407      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747577678.733327      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45bf3098fa3d439097e21923a2ef657b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":16},{"id":"83064ba2","cell_type":"code","source":"train_dataSet = TextClassificationDataset(train_df, tokenizer)\nvalid_dataSet = TextClassificationDataset(valid_df, tokenizer)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataSet, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataSet, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:15:05.098737Z","iopub.execute_input":"2025-05-18T14:15:05.099308Z","iopub.status.idle":"2025-05-18T14:15:05.103989Z","shell.execute_reply.started":"2025-05-18T14:15:05.099285Z","shell.execute_reply":"2025-05-18T14:15:05.103412Z"},"trusted":true},"outputs":[],"execution_count":17},{"id":"70d21a2b","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:15:06.374116Z","iopub.execute_input":"2025-05-18T14:15:06.374931Z","iopub.status.idle":"2025-05-18T14:15:06.698143Z","shell.execute_reply.started":"2025-05-18T14:15:06.374906Z","shell.execute_reply":"2025-05-18T14:15:06.697408Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":18},{"id":"6fdabf44-8b1e-4b84-93e8-95a55ada0850","cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"74da51be","cell_type":"code","source":"from torch.optim import Adam\n\noptimizer = Adam(model.parameters(), 2e-5)","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:15:10.232670Z","iopub.execute_input":"2025-05-18T14:15:10.233446Z","iopub.status.idle":"2025-05-18T14:15:10.238623Z","shell.execute_reply.started":"2025-05-18T14:15:10.233413Z","shell.execute_reply":"2025-05-18T14:15:10.238046Z"},"trusted":true},"outputs":[],"execution_count":20},{"id":"d4612646","cell_type":"markdown","source":"# Training and eval loop","metadata":{}},{"id":"91136a53","cell_type":"code","source":"num_epochs = 4\nlearning_rate = 2e-5\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    epoch_loss = 0\n    model.train()\n\n    for i, batch in enumerate(train_loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        print(f\" {i + 1}/{len(train_loader)}\")\n    avg_epoch_loss = epoch_loss / len(train_loader)\n    print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n\n    valid_err = 0\n    model.eval()\n    with torch.no_grad():\n        for i, batch in enumerate(valid_loader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            outputs = model(**batch)\n            loss = outputs.loss\n            valid_err += loss.item()\n            print(f\" {i + 1}/{len(valid_loader)}\")\n    avg_valid_loss = valid_err / len(valid_loader)\n    print(f\"Average Validation Loss: {avg_valid_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:15:12.059984Z","iopub.execute_input":"2025-05-18T14:15:12.060806Z","iopub.status.idle":"2025-05-18T14:15:13.957571Z","shell.execute_reply.started":"2025-05-18T14:15:12.060772Z","shell.execute_reply":"2025-05-18T14:15:13.956359Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2254400105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    979\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 )\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    632\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 293.12 MiB is free. Process 3155 has 15.60 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 33.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 293.12 MiB is free. Process 3155 has 15.60 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 33.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":21},{"id":"25358f10","cell_type":"markdown","source":"# Save the model","metadata":{}},{"id":"abfc229e-a24f-4b4c-bd6e-926851099271","cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")\ntorch.save(model, \"model.h5\")","metadata":{"execution":{"iopub.status.busy":"2025-05-18T14:10:17.031245Z","iopub.status.idle":"2025-05-18T14:10:17.031480Z","shell.execute_reply.started":"2025-05-18T14:10:17.031357Z","shell.execute_reply":"2025-05-18T14:10:17.031370Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"9d380916-9410-4154-8325-38047ad89941","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}